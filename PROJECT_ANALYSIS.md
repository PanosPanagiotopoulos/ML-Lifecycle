# ML Lifecycle Project Analysis (Report Version)

## 1) Executive Summary

This repository implements an end-to-end ML lifecycle for a course-guide assistant with two production-facing pillars:

- **Model training and promotion pipeline** (`services/train`) that ingests PDF material, fine-tunes a base LLM with LoRA, evaluates it, logs artifacts/metrics to MLflow, and promotes a model version to the **Production** stage.
- **OpenAI-style inference API** (`services/api`) that serves the promoted model (or a local model in development) through `/v1/models` and `/v1/chat/completions`.

The stack is orchestrated through Docker Compose with a shared `data/` volume strategy and MLflow as the model registry and artifact backend.

---

## 2) Project Scope and Objectives

### Core objectives achieved

1. Ingest and preprocess university/course PDFs into training-ready text chunks.
2. Fine-tune a causal LM using LoRA to reduce compute and storage cost.
3. Track experiments and artifacts in MLflow.
4. Register and automatically promote trained models to Production.
5. Expose a lightweight OpenAI-compatible API interface for client integration.

### Current serving modes

- **Registry mode (default/production)**: pull artifacts from MLflow model registry.
- **Local mode (development/testing)**: load model artifacts from a local filesystem directory.

---

## 3) High-Level Architecture

### Services

- `mlflow` service: tracking + model registry + artifact root (`./data/mlruns`).
- `train` service: offline/periodic training pipeline.
- `api` service: online inference and OpenAI-style contract.

### Orchestration

- `docker-compose.yml` defines inter-service networking and shared volume mounts for:
  - `./data/raw_pdfs`
  - `./data/processed`
  - `./data/logs`
  - service config/env files

### Entry points

- Training: `services/train/src/run_pipeline.py`
- API (service container): `services/api/app/main.py`
- API (repo-root local run): `main.py` (shim that exposes `app` for `uvicorn main:app`)

---

## 4) Training Project Analysis (`services/train`)

### 4.1 Configuration and environment model

`services/train/src/config.py` uses layered configuration:

1. Base JSON (`config/settings.json`)
2. Environment JSON override (`settings.development.json` / `settings.production.json`)
3. `.env` profile file (`.env.local` or `.env.docker`, auto-selected)
4. Process environment variables (highest precedence)

Key train parameters include data paths, chunking parameters, LoRA hyperparameters, optimizer/training control parameters, and MLflow endpoints.

### 4.2 Data ingestion and dataset construction

- `ingest_pdfs.py`
  - Recursively scans `raw_pdfs_dir` for PDFs.
  - Extracts per-page text using `pdfplumber`.
  - Collects rows as `{doc_id, text}` with per-document fault tolerance.

- `chunking.py`
  - Normalizes whitespace and soft hyphen artifacts.
  - Splits text by character window with overlap (`chunk_size`, `chunk_overlap`).

- `dataset_builder.py`
  - Builds `train.jsonl` for model training.
  - Also emits `chunks.jsonl` as a chunk-indexed text corpus (currently no longer consumed by API retrieval logic, but still generated by the training side).

### 4.3 Fine-tuning and experiment tracking

- `train_lora.py`
  - Loads base model snapshot from Hugging Face Hub (`snapshot_download`) with optional token.
  - Tokenizes and splits data into train/eval sets.
  - Applies LoRA (`peft`) for causal LM adaptation.
  - Uses hardware-aware precision choices (`fp16`/`bf16` where available).
  - Trains via `transformers.Trainer`, evaluates, and logs:
    - parameters
    - `eval_loss`
    - derived perplexity
    - merged deployable model artifacts

### 4.4 Model registry lifecycle

- `mlflow_registry.py`
  - Ensures registered model existence.
  - Registers model from run artifacts.
  - Has fallback path (`create_model_version`) if direct register fails.
  - Waits for model version readiness.
  - Promotes latest version to **Production** and archives previous versions.

### 4.5 Pipeline orchestration

- `run_pipeline.py`
  - Drives ingest → dataset build → training sequence.
  - Enforces early abort when no documents or empty dataset.
  - Provides phase-level logging through `logging_service.py`.

---

## 5) API Project Analysis (`services/api`)

### 5.1 API contract and routing

- `GET /health`
  - Returns service status and model readiness.

- `GET /v1/models`
  - Returns model inventory in OpenAI-style list shape.

- `POST /v1/chat/completions`
  - Accepts OpenAI-like message arrays (`system|user|assistant`).
  - Uses latest user message as the question.
  - Uses concatenated system messages as optional contextual instructions.
  - Returns OpenAI-style completion object with usage and latency metadata.

### 5.2 Error handling standardization

`services/api/app/main.py` defines global exception handling:

- `RequestValidationError` → standardized 422 payload with per-field details.
- `HTTPException` → standardized payload with status-specific professional messages.
- uncaught `Exception` → standardized 500 payload + server-side logging.

All API errors are returned in a unified schema:

```json
{
  "error": {
    "code": "...",
    "message": "...",
    "status": 422,
    "path": "/v1/chat/completions",
    "details": []
  }
}
```

### 5.3 Configuration and deployment modes

`services/api/app/config.py` supports layered config and path normalization similar to train.

Serving mode is controlled by `MODEL_SOURCE`:

- `registry`: resolve model from MLflow registry stage (`models:/.../Production` by default).
- `local`: resolve model from `LOCAL_MODEL_DIR`.

Additional logic auto-rewrites `mlflow` hostname to `localhost` when running outside Docker and DNS does not resolve `mlflow`.

### 5.4 LLM serving internals

- `services/api/app/services/llm_model_service.py`
  - Loads/merges LoRA adapters when adapter artifacts are detected.
  - Caches loaded model/tokenizer per model id.
  - Caches registry model list for short TTL (10 seconds).
  - Applies generation safeguards:
    - capped `max_new_tokens`
    - optional sampling based on temperature
    - repetition controls (`repetition_penalty`, `no_repeat_ngram_size`)
    - post-processing to suppress repetitive `Question:/Answer:` loops.

### 5.5 OpenAI compatibility level

The API is **OpenAI-style compatible** for core chat/model endpoints and payload structure, but is intentionally minimal:

- implemented: `/v1/models`, `/v1/chat/completions`
- not implemented: streaming responses, tools/function-calling, embeddings, assistants, responses API variants.

---

## 6) Data and Artifact Lifecycle

### Input

- Raw source files: `data/raw_pdfs/` (mounted into train container).

### Intermediate outputs

- `data/processed/train.jsonl`
- `data/processed/chunks.jsonl`
- temporary trainer checkpoints in `data/processed/artifacts_tmp/`

### Deployable outputs

- merged model artifacts in `data/processed/model_out/`
- tracked run artifacts and metadata in `data/mlruns/`

### Runtime logs

- service logs under `data/logs/`

---

## 7) Dependency Profile

### Train-focused stack

- `transformers`, `peft`, `datasets`, `torch`, `accelerate`
- `pdfplumber` for ingestion
- `mlflow` for lifecycle tracking

### API-focused stack

- `fastapi`, `uvicorn`
- `transformers`, `torch`, `peft` for inference
- `mlflow` for registry-backed model retrieval

Root `requirements.txt` consolidates broader project dependencies; service-level requirements keep container builds focused.

---

## 8) Feature Inventory (Current State)

### Training features

- Recursive PDF ingestion and extraction.
- Configurable chunking pipeline.
- JSONL training dataset builder.
- LoRA fine-tuning with evaluation.
- MLflow experiment tracking.
- Automatic model registration and Production promotion.

### API features

- OpenAI-style model listing and chat completion endpoints.
- Registry and local model source switching.
- Unified, descriptive REST error schema.
- Health/readiness checks.
- Per-model and registry-list caching.
- Anti-repetition generation safeguards.

### Explicitly removed behavior

- Automatic retrieval-augmented chunk lookup in API response generation has been removed from serving logic.
- API can still include context when provided via `system` messages in chat payloads.

---

## 9) Strengths, Gaps, and Risks

### Strengths

- Clear separation of training and serving responsibilities.
- Practical production/development config layering.
- Registry-centric lifecycle with promotion semantics.
- Standardized API error contract.

### Gaps / technical debt

1. **Config drift**: API `.env` files still contain legacy retrieval variables (`PROCESSED_CHUNKS_PATH`, `RETRIEVAL_TOP_K`) not consumed by current API config model.
2. **Security risk**: `services/train/.env.docker` currently contains a concrete `HF_TOKEN` value; this should be rotated and moved to secure secret management.
3. **Interface naming residue**: function name `get_llm_model_service` is current, but some log naming still uses `api.inference` and can be normalized for consistency.
4. **OpenAI parity scope** is partial; advanced endpoint families are not implemented.

### Operational risks

- Large model loading can increase cold-start latency.
- Local development path mismatches can occur if env/config overrides are inconsistent.
- Training pipeline currently assumes enough memory/compute for base model loading + LoRA operations.

---

## 10) Recommended Improvements (Report-Friendly Roadmap)

### Near-term (high value, low effort)

1. Remove unused legacy env keys from API `.env` files.
2. Rotate exposed Hugging Face token and migrate to secret injection.
3. Add OpenAPI response component schemas for all standardized error responses.
4. Add smoke tests for `/health`, `/v1/models`, `/v1/chat/completions` against a small local model fixture.

### Mid-term

1. Add structured request/response logging with correlation IDs.
2. Add load-time model warmup and health degradation reasons with machine-readable codes.
3. Add CI checks for config schema drift between `.env` and Pydantic models.

### Longer-term

1. Add streaming completion support and stricter OpenAI API parity if required.
2. Introduce evaluation benchmark suite (quality + latency) per promoted model version.
3. Add governance metadata (dataset/model lineage report generation per run).

---

## 11) How to Run

### Local API (from repository root)

```bash
uvicorn main:app --reload --host 127.0.0.1 --port 8000
```

### Local training pipeline

```bash
python services/train/src/run_pipeline.py
```

### Full Docker stack

```bash
docker compose up --build
```

---

## 12) Conclusion

The project is a solid implementation of an ML lifecycle pattern combining offline fine-tuning and online model serving through a practical, registry-driven MLOps architecture. The current codebase is functionally cohesive and report-ready, with key follow-up actions focused on security hygiene, config cleanup, and deeper API parity/testing.
